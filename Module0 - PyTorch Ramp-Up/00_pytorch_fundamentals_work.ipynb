{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOiS2foNIx6KdQUe3ZVQrSR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanish-Sarkar/Elite-Transformers/blob/main/Module0%20-%20PyTorch%20Ramp-Up/00_pytorch_fundamentals_work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Tensor making**"
      ],
      "metadata": {
        "id": "hHpMZBRrZaAo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rP7_gv9wZTaI",
        "outputId": "8510454a-65af-4f13-913f-88060cc261ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor in device:  cuda:0\n",
            "Matmul shape: torch.Size([5, 5])\n",
            "Reshaped: torch.Size([15])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "x = torch.rand(5,3,device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Tensor in device: \", x.device)\n",
        "print(\"Matmul shape:\", torch.matmul(x, x.T).shape)\n",
        "print(\"Reshaped:\", x.view(-1).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Building the nn.Module**"
      ],
      "metadata": {
        "id": "6BCdcJmwaVOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, d_model=64, num_heads=4):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_head = d_model // num_heads\n",
        "    self.qkv_proj = nn.Linear(d_model, d_model * 3) # Corrected line\n",
        "    self.out_proj = nn.Linear(d_model, d_model)\n",
        "    self.scale = self.d_head ** -0.5\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    qkv = self.qkv_proj(x).reshape(B,T,3, self.num_heads, self.d_head)\n",
        "    q,k,v = qkv.unbind(2)\n",
        "    attn = torch.matmul(q,k.transpose(-2, -1)) * self.scale\n",
        "    attn = attn.softmax(dim=-1)\n",
        "    out = torch.matmul(attn, v)\n",
        "    out = out.transpose(1, 2).reshape(B,T,C)\n",
        "    return self.out_proj(out)\n",
        "\n",
        "\n",
        "# Testing\n",
        "model = MultiheadAttention()\n",
        "x = torch.rand(2,10,64)\n",
        "out = model(x)\n",
        "print(\"Multi-Head output shape: \", out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZLuhsCEaBa9",
        "outputId": "9b8d3b7b-5223-4392-bfbb-b331ccf97f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-Head output shape:  torch.Size([2, 10, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. NLP**"
      ],
      "metadata": {
        "id": "yljTszQtdNoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "text = requests.get(\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\").text[:50000]\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, block_size=128):\n",
        "        self.data = [stoi[c] for c in data if c in stoi]\n",
        "        self.block_size = block_size\n",
        "    def __len__(self): return len(self.data) - self.block_size\n",
        "    def __getitem__(self, i):\n",
        "        chunk = self.data[i:i+self.block_size+1]\n",
        "        return torch.tensor(chunk[:-1]), torch.tensor(chunk[1:])\n",
        "\n",
        "dataset = CharDataset(text)\n",
        "loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "xb, yb = next(iter(loader))\n",
        "print(\"Batch shapes:\", xb.shape, yb.shape)\n",
        "print(\"Sample text:\", ''.join(itos[i.item()] for i in xb[0][:50]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM3Iwj3ac5Iv",
        "outputId": "a1e2c051-78d2-4d0e-e4a3-a19335b266b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch shapes: torch.Size([8, 128]) torch.Size([8, 128])\n",
            "Sample text: those\n",
            "That best can aid your action.\n",
            "\n",
            "MARCIUS:\n",
            "Tho\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HsZo6QsGg2-m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}