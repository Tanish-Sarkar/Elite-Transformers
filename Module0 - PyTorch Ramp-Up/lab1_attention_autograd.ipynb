{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanish-Sarkar/Elite-Transformers/blob/main/Module0%20-%20PyTorch%20Ramp-Up/lab1_attention_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz0MkvHnvf_G",
        "outputId": "ffd07a6a-0193-4ab1-ac4a-dfe3d15179fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78dfd4237330>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create \"word embeddings\""
      ],
      "metadata": {
        "id": "vntCg30Hws6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate 10 words, embedding size = 512\n",
        "embedding = torch.rand(10, 512, requires_grad=True) # Autograd enabled\n",
        "\n",
        "# Dummy projection matrices\n",
        "W_q = torch.rand(512,512, requires_grad=True)\n",
        "W_k = torch.rand(512,512, requires_grad=True)\n",
        "\n",
        "Q = embedding @ W_q\n",
        "K = embedding @ W_k"
      ],
      "metadata": {
        "id": "AIQj2q59wINa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Dummy attention (Q @ Káµ€)"
      ],
      "metadata": {
        "id": "8Q9bOLxyxvcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = Q @ K.T\n",
        "attn = torch.softmax(scores, dim=1)"
      ],
      "metadata": {
        "id": "npweDsqnxraP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Autograd test"
      ],
      "metadata": {
        "id": "T6qbywVBx7lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero out gradients before backward pass to prevent accumulation\n",
        "if embedding.grad is not None:\n",
        "    embedding.grad.zero_()\n",
        "if W_q.grad is not None:\n",
        "    W_q.grad.zero_()\n",
        "if W_k.grad is not None:\n",
        "    W_k.grad.zero_()\n",
        "\n",
        "target = torch.ones_like(attn)\n",
        "loss = (attn - target).pow(2).mean()\n",
        "loss.backward(retain_graph=True)\n",
        "\n",
        "print(\"Loss: \", loss.item())\n",
        "print(\"Grad embedding[0][0]: \", embedding.grad[0][0])\n",
        "print(\"Grad W_q[0][0]: \", W_q.grad[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-3nQzmEx4vh",
        "outputId": "e230a809-4c7e-4551-bb47-8be5c0a62daf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  0.8999999761581421\n",
            "Grad embedding[0][0]:  tensor(0.)\n",
            "Grad W_q[0][0]:  tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. NLP tie-in: tokenization + embedding"
      ],
      "metadata": {
        "id": "tsgknlH8zFQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {\n",
        "    \"hello\" : 1,\n",
        "    \"world\" : 2\n",
        "}\n",
        "sentence = ['hello', 'world']\n",
        "\n",
        "token_ids = torch.tensor([vocab[w] for w in sentence])\n",
        "\n",
        "embed_layer = nn.Embedding(num_embeddings=100, embedding_dim=5)\n",
        "embedding_sentence = embed_layer(token_ids)\n",
        "\n",
        "print(\"Token IDs: \", token_ids)\n",
        "print(\"Embedding sentence Shape: \", embedding_sentence.shape)\n"
      ],
      "metadata": {
        "id": "6hWxt1NcyFRI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c0dcb7-bf56-4e6e-a33c-9e40ddd36213"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:  tensor([1, 2])\n",
            "Embedding sentence Shape:  torch.Size([2, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qfolRBVm4Inp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}